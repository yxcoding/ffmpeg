/* DO NOT EDIT THIS FILE - it is machine generated */
#include <jni.h>
#include "string"
#include <android/log.h>
#include <android/native_window.h>
#include <android/native_window_jni.h>
#include "unistd.h"

/* Header for class net_yxcoding_ffmpeg_FFmpegUtil */

#define LOGE(...) __android_log_print( ANDROID_LOG_ERROR, "yxcoding", __VA_ARGS__ )
#define LOGD(...) __android_log_print( ANDROID_LOG_DEBUG, "yxcoding", __VA_ARGS__ )

#ifndef _Included_net_yxcoding_ffmpeg_FFmpegUtil
#define _Included_net_yxcoding_ffmpeg_FFmpegUtil
#ifdef __cplusplus
extern "C" {
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <libavutil/imgutils.h>
#include <libswscale/swscale.h>
#include <libyuv.h>
#include <libavutil/time.h>
#include <libsdl/SDL.h>

#endif

#define EVENT_FTP_REFRESH 0x1
#define EVENT_BREAK (EVENT_FTP_REFRESH + 1)

#define MODE 1


void decodeVide(AVFormatContext *avFormatContext, JNIEnv *env, jobject jsurface);

JNIEXPORT jstring JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_ffmpegInfo
        (JNIEnv *, jclass) {

}

JNIEXPORT void JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_videoInfo
        (JNIEnv *env, jclass, jstring jfilePath, jstring joutFile) {

}

void formatTime(int64_t time) {
    int hours, mins, secs, us;
    int64_t duration = time;
    secs = duration / AV_TIME_BASE;
    us = duration % AV_TIME_BASE;
    mins = secs / 60;
    secs %= 60;
    hours = mins / 60;
    mins %= 60;
    //LOGD("%02d:%02d:%02d.%02d\n", hours, mins, secs, (100 * us) / AV_TIME_BASE);
    LOGD("%02d:%02d:%02d\n", hours, mins, secs);
}

void play(const char *filePath) {
    LOGD("filePath = %s", filePath);

    // ffmpeg 第一步初始化注册
    av_register_all();

    // 获取上下文路径
    AVFormatContext *avFormatContext = avformat_alloc_context();

    // 打开媒体文件
    if (avformat_open_input(&avFormatContext, filePath, NULL, NULL) != 0) {
        LOGE("%s", "无法打开视频文件");
        return;
    }

    // 获取媒体流信息
    if (avformat_find_stream_info(avFormatContext, NULL) < 0) {
        LOGE("%s", "无法获取视频信息");
        return;
    }
    //SDL_SetMainReady();
    //------------SDL初始化--------
    if(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
        LOGE( "Could not initialize SDL - %s\n", SDL_GetError());
        return;
    }


    decodeVide(avFormatContext, NULL, NULL);

    SDL_Quit();
    avformat_close_input(&avFormatContext);

    avformat_free_context(avFormatContext);
}

JNIEXPORT void JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_playVideo
        (JNIEnv *env, jclass, jstring jfilePath, jobject jsurface) {

    const char *filePath = env->GetStringUTFChars(jfilePath, JNI_FALSE);
    LOGD("filePath = %s", filePath);

    // ffmpeg 第一步初始化注册
    av_register_all();

    // 获取上下文路径
    AVFormatContext *avFormatContext = avformat_alloc_context();

    // 打开媒体文件
    if (avformat_open_input(&avFormatContext, filePath, NULL, NULL) != 0) {
        LOGE("%s", "无法打开视频文件");
        return;
    }

    // 获取媒体流信息
    if (avformat_find_stream_info(avFormatContext, NULL) < 0) {
        LOGE("%s", "无法获取视频信息");
        return;
    }
    //------------SDL初始化--------
    if(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER)) {
        LOGE( "Could not initialize SDL - %s\n", SDL_GetError());
        return;
    }


    decodeVide(avFormatContext, env, jsurface);


    avformat_close_input(&avFormatContext);

    avformat_free_context(avFormatContext);
}
int thread_exit;
int ftp_refresh(void *opaque) {
    thread_exit=0;
    while (!thread_exit) {
        SDL_Event event;
        event.type = EVENT_FTP_REFRESH;
        SDL_PushEvent(&event);
        //SDL_Delay(20);
    }
    thread_exit=0;
    //Break
    SDL_Event event;
    event.type = EVENT_BREAK;
    SDL_PushEvent(&event);

    return 0;
}

/**
 * 视频解码
 * @param avFormatContext
 * @param env
 * @param jsurface
 */
void decodeVide(AVFormatContext *avFormatContext, JNIEnv *env, jobject jsurface) {
    LOGD("nb_streams = %d", avFormatContext->nb_streams);

    int videoIndex = -1;
    // 获取视频流index
    for (int i = 0; i < avFormatContext->nb_streams; i++) {
        if (avFormatContext->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {
            videoIndex = i;
            break;
        }
    }
    LOGD("videoIndex = %d", videoIndex);

    formatTime(avFormatContext->duration);

    // 获取解码器参数
    AVCodecParameters *codecpar = avFormatContext->streams[videoIndex]->codecpar;
    LOGD("video width = %d, video height = %d", codecpar->width, codecpar->height);

    // 获取解码器
    AVCodec *avCodec = avcodec_find_decoder(codecpar->codec_id);
    if (avCodec == NULL) {
        LOGE("%s", "无法找到解码器");
        return;
    }
    AVCodecContext *avCodecContext = avFormatContext->streams[videoIndex]->codec;
    // 打开解码器
    if (avcodec_open2(avCodecContext, avCodec, NULL) < 0) {
        LOGE("%s", "无法打开解码器");
        return;
    }

    double w_h = codecpar->width * 1.0f / codecpar->height * 1.0f;

    LOGD("w_h = %f", w_h);

    //int outWidth = codecpar->width / 2;
    //int outHeight = outWidth / w_h;
    int outWidth = codecpar->width;
    int outHeight = codecpar->height;


    LOGD("outHeight = %d", outHeight);

    // 获取native window
    //ANativeWindow *nativeWindow = ANativeWindow_fromSurface(env, jsurface);
    //ANativeWindow_Buffer windowBuffer;


    SDL_Window *screen = SDL_CreateWindow("Player",
                                          SDL_WINDOWPOS_UNDEFINED,SDL_WINDOWPOS_UNDEFINED,
                                          0, 0,SDL_WINDOW_RESIZABLE|SDL_WINDOW_OPENGL);
    if(!screen) {
        LOGE("SDL: could not set video mode - exiting\n");
        return;
    }

    //SDL_SetWindowSize(screen, codecpar->width, codecpar->height);

    int w=0;
    int h=0;
    SDL_GetWindowSize(screen,&w,&h);

    LOGE("SDL_GetWindow w = %d, h = %d", w,h);
#if MODE
    Uint32 sdl_out_fmt = SDL_PIXELFORMAT_IYUV;
#else
    Uint32 sdl_out_fmt = SDL_PIXELFORMAT_RGBA32;
#endif

    SDL_Renderer* sdlRenderer = SDL_CreateRenderer(screen, -1, 0);
    SDL_Texture* sdlTexture = SDL_CreateTexture(
            sdlRenderer,
            sdl_out_fmt,
            SDL_TEXTUREACCESS_STREAMING,
            outWidth,
            outHeight);
    SDL_Event event;

    SDL_Rect rect;
    SDL_Rect rect2;

    SDL_Rect rect3;
    SDL_Rect rect4;

    // 编码数据（压缩数据）
    AVPacket *packet = (AVPacket *) av_malloc(sizeof(AVPacket));    // 为一帧图像分配内存
    // 解码数据
    // Y    U V  4:1:1
    // 亮度  色度
    AVFrame *avFrame = av_frame_alloc();
    AVFrame *avOutFrame = av_frame_alloc();

    if (avFrame == NULL || avOutFrame == NULL) {
        LOGE("%s", "frame alloc fail");
        return;
    }

#if MODE
    //output frame for SDL
    enum AVPixelFormat pixel_fmt = AV_PIX_FMT_YUV420P;

#else
    //output RGBFrame
    enum AVPixelFormat pixel_fmt = AV_PIX_FMT_RGBA;
#endif

    //size buffer
    uint8_t *out_buffer = (uint8_t *)
            av_malloc((size_t) av_image_get_buffer_size(pixel_fmt, codecpar->width, codecpar->height, 1));
    av_image_fill_arrays(avOutFrame->data,avOutFrame->linesize, out_buffer,
                         pixel_fmt,codecpar->width,codecpar->height,1);

    //int winWidth = ANativeWindow_getWidth(nativeWindow);
    //int winHeight = ANativeWindow_getHeight(nativeWindow);

    struct SwsContext *swsContext = sws_getContext(codecpar->width, codecpar->height,
                                                   avCodecContext->pix_fmt,
                                                   codecpar->width, codecpar->height,
                                                   pixel_fmt,
                                                   SWS_BICUBIC, NULL, NULL, NULL);

    SDL_CreateThread(ftp_refresh,NULL,NULL);

    AVStream *videoStream = avFormatContext->streams[videoIndex];

    int frame_rate=videoStream->avg_frame_rate.num/videoStream->avg_frame_rate.den;//每秒多少帧
    LOGD("frame_rate = %d", frame_rate);

    FILE *yuvFile = fopen("/sdcard/12345.yuv", "wb+");
    for(;;) {
        SDL_WaitEvent(&event);
        if(EVENT_FTP_REFRESH == event.type) {
            if(av_read_frame(avFormatContext, packet) >= 0) {
                // 对视频进行解码
                if (packet->stream_index == videoIndex) {
                    //avcodec_decode_video2();
                    int res = avcodec_send_packet(avCodecContext, packet);
                    if (res != 0) {
                        av_packet_unref(packet);
                        LOGE("decode error %d %d %d %d %d", res, AVERROR(EAGAIN), AVERROR_EOF,
                             AVERROR(EINVAL),
                             AVERROR(ENOMEM));
                        continue;
                    }
                    if (avcodec_receive_frame(avCodecContext, avFrame) != 0) {
                        av_packet_unref(packet);
                        LOGE("%s", "decode error2.\n");
                        continue;
                    }

                    //double time = packet->pts * av_q2d(videoStream->time_base);
                    //formatTime(time * AV_TIME_BASE);


                    //ANativeWindow_setBuffersGeometry(nativeWindow, outWidth, outHeight,
                    //WINDOW_FORMAT_RGBA_8888);
                    // lock native window buffer
                    //ANativeWindow_lock(nativeWindow, &windowBuffer, NULL);

                    // 设置rgb_frame 的属性（像素格式，宽高）和缓冲区
                    // rgb_frame 缓冲区与windowBuffer.bits 是同一块内存，指向surface
                    //avpicture_fill((AVPicture *) avOutFrame, (const uint8_t *) windowBuffer.bits,
                    //AV_PIX_FMT_RGBA, outWidth, outHeight);

                    // YUV420 > RGB
                    /*libyuv::I420ToARGB(avFrame->data[0], avFrame->linesize[0],
                                       avFrame->data[2], avFrame->linesize[2],
                                       avFrame->data[1], avFrame->linesize[1],
                                       avOutFrame->data[0], avOutFrame->linesize[0],
                                       outWidth, outHeight);*/

                   sws_scale(swsContext, (const uint8_t *const *) avFrame->data, avFrame->linesize,
                              0,
                              codecpar->height,
                              avOutFrame->data, avOutFrame->linesize);
                    //------------SDL显示--------
                    int margin = w - outWidth;
                    ///LOGD("margin = %d", margin);
                    rect.x = 0;
                    rect.y = 0;

                #if MODE
                    rect.w = w;
                    rect.h = h;

                #else
                    rect.w = outWidth;
                    rect.h = outHeight;
                #endif
                    rect2.x = outWidth;
                    rect2.y = 0;
                    rect2.w = outWidth;
                    rect2.h = outHeight;

                    rect3.x = 0;
                    rect3.y = outHeight;
                    rect3.w = outWidth;
                    rect3.h = outHeight;

                    rect4.x = outWidth;
                    rect4.y = outHeight;
                    rect4.w = outWidth;
                    rect4.h = outHeight;

                    //LOGD("rect.w = %d, rect.h = %d", rect.w,rect.h);
                #if MODE
                    SDL_UpdateYUVTexture(sdlTexture, NULL, avOutFrame->data[0], avOutFrame->linesize[0],
                                         avOutFrame->data[1], avOutFrame->linesize[1],
                                         avOutFrame->data[2],avOutFrame->linesize[2]);

                    int y_size=codecpar->width*codecpar->height;
                    fwrite(avOutFrame->data[0],1,y_size,yuvFile);    //Y
                    fwrite(avOutFrame->data[1],1,y_size/4,yuvFile);  //U
                    fwrite(avOutFrame->data[2],1,y_size/4,yuvFile);  //V
                #else
                    SDL_UpdateTexture( sdlTexture, &rect, avOutFrame->data[0], avOutFrame->linesize[0]);
                #endif


//            SDL_UpdateTexture( sdlTexture, &rect2, avOutFrame->data[0], avOutFrame->linesize[0]);
//            SDL_UpdateTexture( sdlTexture, &rect3, avOutFrame->data[0], avOutFrame->linesize[0]);
//            SDL_UpdateTexture( sdlTexture, &rect4, avOutFrame->data[0], avOutFrame->linesize[0]);

                    SDL_RenderClear( sdlRenderer );

                    SDL_RenderCopy( sdlRenderer, sdlTexture, &rect, &rect );
//            SDL_RenderCopy( sdlRenderer, sdlTexture, NULL, &rect2 );
//            SDL_RenderCopy( sdlRenderer, sdlTexture, NULL, &rect3 );
//            SDL_RenderCopy( sdlRenderer, sdlTexture, NULL, &rect4 );

                    SDL_RenderPresent( sdlRenderer );
                    //延时20ms
                    //------------SDL-----------
SDL_Delay(40);
                    // ANativeWindow_unlockAndPost(nativeWindow);
                }
                av_packet_unref(packet);
            }
            else{
                thread_exit = 1;
                break;
            }
        }
        else if(SDL_QUIT == event.type) {
            thread_exit = 1;
        }
        else if(EVENT_BREAK == event.type) {
            break;
        }
    }

  /*  while (av_read_frame(avFormatContext, packet) >= 0) {

    }*/

    fclose(yuvFile);
    SDL_DestroyTexture(sdlTexture);

    sws_freeContext(swsContext);

    // 释放资源
    av_frame_free(&avFrame);
    av_frame_free(&avOutFrame);

    avcodec_close(avCodecContext);
}

int main(int argc, char *argv[]) {
    char* filePath = argv[1];
    play(filePath);
    LOGE("%s", "in to main");
    return 0;
}

#ifdef __cplusplus
}
#endif
#endif
