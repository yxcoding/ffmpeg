/* DO NOT EDIT THIS FILE - it is machine generated */
#include <jni.h>
#include "string"
#include <android/log.h>
#include <android/native_window.h>
#include <android/native_window_jni.h>
#include "unistd.h"
/* Header for class net_yxcoding_ffmpeg_FFmpegUtil */

#define LOGE(...) __android_log_print( ANDROID_LOG_ERROR, "yxcoding", __VA_ARGS__ )
#define LOGD(...) __android_log_print( ANDROID_LOG_DEBUG, "yxcoding", __VA_ARGS__ )

#ifndef _Included_net_yxcoding_ffmpeg_FFmpegUtil
#define _Included_net_yxcoding_ffmpeg_FFmpegUtil
#ifdef __cplusplus
extern "C" {
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <libavutil/imgutils.h>
#include <libswscale/swscale.h>
#include <libyuv.h>
#include <libavutil/time.h>
#endif

JNIEXPORT jstring JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_ffmpegInfo
        (JNIEnv *, jclass) {

}

JNIEXPORT void JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_videoInfo
        (JNIEnv *env, jclass, jstring jfilePath, jstring joutFile) {

}

void formatTime(int64_t time) {
    int hours, mins, secs, us;
    int64_t duration = time;
    secs = duration / AV_TIME_BASE;
    us = duration % AV_TIME_BASE;
    mins = secs / 60;
    secs %= 60;
    hours = mins / 60;
    mins %= 60;
    //LOGD("%02d:%02d:%02d.%02d\n", hours, mins, secs, (100 * us) / AV_TIME_BASE);
    LOGD("%02d:%02d:%02d\n", hours, mins, secs);
}

int ScaleYUVImgToRGB(int nSrcW, int nSrcH, uint8_t *src_data, int *linesize, int nDstW, int nDstH) {
    int i;
    int ret;
    FILE *nRGB_file;

    AVFrame *nDst_picture;
    struct SwsContext *m_pSwsContext;

    nDst_picture = av_frame_alloc();
    if (!nDst_picture) {
        printf("nDst_picture avcodec_alloc_frame failed\n");
        exit(1);
    }
    if (avpicture_alloc((AVPicture *) nDst_picture, AV_PIX_FMT_RGBA, nDstW, nDstH) < 0) {
        printf("dst_picture avpicture_alloc failed\n");
        return -1;
    }
    m_pSwsContext = sws_getContext(nSrcW, nSrcH, AV_PIX_FMT_YUV420P,
                                   nDstW, nDstH, AV_PIX_FMT_RGBA,
                                   SWS_BICUBIC,
                                   NULL, NULL, NULL);

    if (NULL == m_pSwsContext) {
        printf("ffmpeg get context error!\n");
        exit(-1);
    }

    ret = sws_scale(m_pSwsContext, (const uint8_t *const *) src_data, linesize, 0, nSrcH,
                    nDst_picture->data,
                    nDst_picture->linesize);

    nRGB_file = fopen("..\\YUV_STREAM\\RGBFile.rgb", "ab+");
    fwrite(nDst_picture->data[0], nDstW * nDstH * 3, 1, nRGB_file);
    fclose(nRGB_file);

    sws_freeContext(m_pSwsContext);
    avpicture_free((AVPicture *) nDst_picture);

    return 0;
}

JNIEXPORT void JNICALL Java_net_yxcoding_ffmpeg_FFmpegUtil_playVideo
        (JNIEnv *env, jclass, jstring jfilePath, jobject jsurface) {

    const char *filePath = env->GetStringUTFChars(jfilePath, JNI_FALSE);
    LOGD("filePath = %s", filePath);

    // ffmpeg 第一步初始化注册
    av_register_all();

    // 获取上下文路径
    AVFormatContext *avFormatContext = avformat_alloc_context();

    // 打开媒体文件
    if (avformat_open_input(&avFormatContext, filePath, NULL, NULL) != 0) {
        LOGE("%s", "无法打开视频文件");
        return;
    }

    // 获取媒体流信息
    if (avformat_find_stream_info(avFormatContext, NULL) < 0) {
        LOGE("%s", "无法获取视频信息");
        return;
    }

    LOGD("nb_streams = %d", avFormatContext->nb_streams);

    int videoIndex = -1;
    // 获取视频流index
    for (int i = 0; i < avFormatContext->nb_streams; i++) {
        if (avFormatContext->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {
            videoIndex = i;
            break;
        }
    }
    LOGD("videoIndex = %d", videoIndex);

    formatTime(avFormatContext->duration);

    // 获取解码器参数
    AVCodecParameters *codecpar = avFormatContext->streams[videoIndex]->codecpar;
    LOGD("video width = %d, video height = %d", codecpar->width, codecpar->height);

    // 获取解码器
    AVCodec *avCodec = avcodec_find_decoder(codecpar->codec_id);
    if (avCodec == NULL) {
        LOGE("%s", "无法找到解码器");
        return;
    }
    //AVCodecContext *avCodecContext = avcodec_alloc_context3(avCodec);
    AVCodecContext *avCodecContext = avFormatContext->streams[videoIndex]->codec;
    // 打开解码器
    if (avcodec_open2(avCodecContext, avCodec, NULL) < 0) {
        LOGE("%s", "无法打开解码器");
        return;
    }

    double w_h = codecpar->width * 1.0f / codecpar->height * 1.0f;

    LOGD("w_h = %f", w_h);

    //int outWidth = codecpar->width / 2;
    //int outHeight = outWidth / w_h;
    int outWidth = codecpar->width;
    int outHeight = codecpar->height;


    LOGD("outHeight = %d", outHeight);

    // 获取native window
    ANativeWindow *nativeWindow = ANativeWindow_fromSurface(env, jsurface);
    ANativeWindow_Buffer windowBuffer;

    // 获得帧图片大小
    //int acImageBufferSize = av_image_get_buffer_size(AV_PIX_FMT_RGBA, outWidth, outHeight, 1);

    //LOGD("acImageBufferSize = %d", acImageBufferSize);

    // uint8_t *buf;
    //buf = (uint8_t *) av_malloc(acImageBufferSize);

//    if (buf == NULL) {
//        LOGE("%s", "av malloc fail");
//        return;
//    }


    // 编码数据（压缩数据）
    AVPacket *packet = (AVPacket *) av_malloc(sizeof(AVPacket));    // 为一帧图像分配内存
    // 解码数据
    // Y    U V  4:1:1
    // 亮度  色度
    AVFrame *avYUVFrame = av_frame_alloc();
    AVFrame *avRGBFrame = av_frame_alloc();
    AVFrame *avRGBScaleFrame = av_frame_alloc();

    if (avYUVFrame == NULL || avRGBFrame == NULL) {
        LOGE("%s", "frame alloc fail");
        return;
    }
    /*av_image_fill_arrays(avRGBFrame->data, avRGBFrame->linesize, buf, AV_PIX_FMT_RGBA,
                         outWidth, outHeight, 1);*/

    //avpicture_fill()

    LOGE("%d", avCodecContext->pix_fmt);

    int winWidth = ANativeWindow_getWidth(nativeWindow);
    int winHeight = ANativeWindow_getHeight(nativeWindow);

    struct SwsContext *swsContext = sws_getContext(codecpar->width, codecpar->height,
                                                   AV_PIX_FMT_YUV420P,
                                                   outWidth, outHeight,
                                                   AV_PIX_FMT_RGBA,
                                                   SWS_BICUBIC, NULL, NULL, NULL);

    LOGE("%d", avCodecContext->pix_fmt);

    AVStream *videoStream = avFormatContext->streams[videoIndex];

    LOGD("winwidth = %d, winHeight = %d", ANativeWindow_getHeight(nativeWindow),
         ANativeWindow_getWidth(nativeWindow));

    while (av_read_frame(avFormatContext, packet) >= 0) {
        // 对视频进行解码
        if (packet->stream_index == videoIndex) {
            //avcodec_decode_video2();
            int res = avcodec_send_packet(avCodecContext, packet);
            if (res != 0) {
                av_packet_unref(packet);
                LOGE("decode error %d %d %d %d %d", res, AVERROR(EAGAIN), AVERROR_EOF,
                     AVERROR(EINVAL),
                     AVERROR(ENOMEM));
                return;
            }
            if (avcodec_receive_frame(avCodecContext, avYUVFrame) != 0) {
                av_packet_unref(packet);
                LOGE("%s", "decode error2.\n");
                return;
            }

            double time = packet->pts * av_q2d(videoStream->time_base);
            formatTime(time * AV_TIME_BASE);


            ANativeWindow_setBuffersGeometry(nativeWindow, outWidth, outHeight,
                                             WINDOW_FORMAT_RGBA_8888);
            // lock native window buffer
            ANativeWindow_lock(nativeWindow, &windowBuffer, NULL);

            // 设置rgb_frame 的属性（像素格式，宽高）和缓冲区
            // rgb_frame 缓冲区与windowBuffer.bits 是同一块内存，指向surface
            avpicture_fill((AVPicture *) avRGBFrame, (const uint8_t *) windowBuffer.bits,
                           AV_PIX_FMT_RGBA, outWidth, outHeight);

            // YUV420 > RGB
            /*libyuv::I420ToARGB(avYUVFrame->data[0], avYUVFrame->linesize[0],
                               avYUVFrame->data[2], avYUVFrame->linesize[2],
                               avYUVFrame->data[1], avYUVFrame->linesize[1],
                               avRGBFrame->data[0], avRGBFrame->linesize[0],
                               outWidth, outHeight);*/

            /*     libyuv::ARGBScale(avRGBFrame->data[0], avRGBFrame->linesize[0], outWidth,
                                   outHeight, avRGBScaleFrame->data[0],
                                   avRGBScaleFrame->linesize[0],
                                   outWidth / 2, outHeight / 2, libyuv::kFilterNone);*/
            /* libyuv::YUVToARGBScaleClip(avYUVFrame->data[0], avYUVFrame->linesize[0],
                                        avYUVFrame->data[2], avYUVFrame->linesize[2],
                                        avYUVFrame->data[1], avYUVFrame->linesize[1],
             );*/
            sws_scale(swsContext, (const uint8_t *const *) avYUVFrame->data, avYUVFrame->linesize,
                      0,
                      codecpar->height,
                      avRGBFrame->data, avRGBFrame->linesize);

            // 获取stride
            /* uint8_t *dst = (uint8_t *) windowBuffer.bits;
             int dstStride = windowBuffer.stride * 4;

             uint8_t *src = avRGBFrame->data[0];
             int srcStride = avRGBFrame->linesize[0];

             // 由于window的stride和帧的stride不同,因此需要逐行复制
             for (int h = 0; h < codecpar->height; h++) {
                 memcpy(dst + h * dstStride, src + h * srcStride, srcStride);
             }*/

            ANativeWindow_unlockAndPost(nativeWindow);

            usleep(1000 * 16);
            //LOGE("%s", "Succeed to decode 1 frame!\n");
        }
        av_packet_unref(packet);
    }

    sws_freeContext(swsContext);

    // 释放资源
    av_frame_free(&avYUVFrame);
    av_frame_free(&avRGBFrame);

    avcodec_close(avCodecContext);

    avformat_close_input(&avFormatContext);

    avformat_free_context(avFormatContext);
}

#ifdef __cplusplus
}
#endif
#endif
